\documentclass[12pt]{article}
 
 \usepackage[utf8]{inputenc}
 \usepackage[T1]{fontenc}
 \usepackage[french]{babel}
 \usepackage{mathtools, bm}
 \usepackage{amssymb, bm}
 \usepackage{stmaryrd} 
 \usepackage{enumitem}
 \usepackage{array}
 \usepackage{layout}
 \usepackage{pslatex}
 \usepackage{pstricks-add}
 \usepackage{lmodern}
 \usepackage{listings}
 \usepackage{color}
 \usepackage[pdfborder={0 0 0}]{hyperref}
 \usepackage[babel=true]{csquotes}
 \usepackage{amsthm}
 \usepackage{thmtools, varioref}
 \declaretheorem[title = Theorem, style=plain]{theo}
 \newtheorem{plain}{Proposition}[section]
 \usepackage{lmodern}
 \usepackage{tikz}
 \usepackage{setspace}
 
 \usepackage{slashed} % opérateur de Dirac

 \usepackage{mathrsfs} % jolies cursives (mathscr)
 
 % marge
 \newcommand{\marge}[1]{\usepackage[top=#1,bottom=#1+1cm,left=#1,right=#1]{geometry}}
 \newcommand{\smarge}[2]{\usepackage[top=#1,bottom=#1+1cm,left=#1-#2,right=#1]{geometry}}
 \smarge{2.5cm}{0.6cm}
 %\usepackage[top=2cm,bottom=2cm,left=2cm,right=2cm]{geometry}


 % "boîtes"

 \newcounter{defi}[section]
 \newcounter{prop}[section]
 \newcounter{thm}

%\newcommand{\prop}[1]{\begin{minipage}{.8\linewidth}\begin{plain} #1\end{plain}\end{minipage}\\\vspace{0cm}}
\newcommand{\conj}[1]{\begin{minipage}{.8\linewidth}\textbf{Conjecture.} \textit{#1}\end{minipage}\\\vspace{0cm}}
%\newcommand{\defi}[1]{\vspace{0.6cm}\begin{minipage}{.8\linewidth}\textbf{Definition.} \textit{#1}\end{minipage}\\\vspace{0.3cm}}
\newcommand{\defi}[1]{\stepcounter{defi}\paragraph{Définition \arabic{section}.\arabic{defi}.}\textit{\newline #1}\vspace{0.1cm}}
\newcommand{\prop}[1]{\stepcounter{prop}\paragraph{Proposition \arabic{section}.\arabic{prop}.}\textit{\newline #1}\vspace{0.1cm}}
\newcommand{\thm}[2]{\stepcounter{thm}\paragraph{Theorème \arabic{thm}. (#1)}\textit{\newline #2}\vspace{0.1cm}}
\newcommand{\thma}[1]{\stepcounter{thm}\paragraph{Theorème \arabic{thm}.}\textit{\newline #1}\vspace{0.1cm}}
%\newcommand{\defis}[1]{\vspace{0.6cm}\paragraph{Définitions.}\textit{#1}\vspace{0.3cm}}
\newcommand{\dem}[1]{\begin{proof}#1\end{proof}\vspace{0cm}}
\newcommand{\req}[1]{\paragraph{Remarque.}#1\vspace{0.1cm}}
\newcommand{\reqs}[1]{\paragraph{Remarques.}\begin{enumerate}#1\end{enumerate}\vspace{0.1cm}}
\newcommand{\exmp}[1]{\paragraph{Exemple.}#1\vspace{0.1cm}}
\newcommand{\exmps}[1]{\paragraph{Exemples.}\begin{enumerate}#1\end{enumerate}\vspace{0.1cm}}



% left/right
 
 \newcommand{\Iff}[2]{\left[#1\,;\,#2\right]}
 \newcommand{\Ioo}[2]{\left]#1\,;\,#2\right[}
 \newcommand{\Ifo}[2]{\left[#1\,;\,#2\right[}
 \newcommand{\Iof}[2]{\left]#1\,;\,#2\right]}

 \newcommand{\pth}[1]{\left(#1\right)}
 \newcommand{\cro}[1]{\left[#1\right]}
 \newcommand{\acc}[1]{\left\{#1\right\}}
 \newcommand{\abs}[1]{\left|#1\right|}
 \newcommand{\dabs}[1]{\|#1\|}
 \newcommand{\scal}[1]{\left<#1\right>}
 \newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
 \newcommand{\ceil}[1]{\left\lceil#1\right\rceil}
 \newcommand{\dbcro}[1]{\left\llbracket#1\right\rrbracket}


 % pour faire des commentaires cool
 \newcommand{\esp}{\hspace{1cm}}
 \newcommand{\et}{\hspace{1cm}\text{et}\hspace{1cm}}
 \newcommand{\pet}{\hspace{0.5cm}\text{et}\hspace{0.5cm}}
 \newcommand{\ou}{\hspace{1cm}\text{ou}\hspace{1cm}}
 \newcommand{\pou}{\hspace{0.5cm}\text{ou}\hspace{0.5cm}}
 \newcommand{\avec}{\hspace{1cm}\text{avec}\hspace{1cm}}
 \newcommand{\soit}{\hspace{1cm}\text{soit}\hspace{1cm}}
 \newcommand{\comment}[1]{\hspace{0.5cm}\text{#1}\hspace{0cm}}
 \newcommand{\tq}{\hspace{0.25cm}/ \hspace{0.25cm}}
 \newcommand{\qt}[1]{(\,#1\,)\hspace{1cm}}
 \newcommand{\qti}[1]{\,,\hspace{1cm}#1}
 \newcommand{\vg}{,\,}
 
 \newcommand{\ssi}{\hspace{.2cm}\Leftrightarrow\hspace{.2cm}}
 \newcommand{\gssi}{\hspace{1cm}\Leftrightarrow\hspace{1cm}}

 \newcommand{\yolo}{$\color{red}{\textbf{YOLO}}$}


 % Pour la géométrie
 \newcommand{\vect}[1]{\overrightarrow{#1}}
 
 % divers 

 \newcommand{\mat}[1]{\begin{matrix} #1\end{matrix}}
 \newcommand{\pmat}[1]{\begin{pmatrix} #1\end{pmatrix}}
 \newcommand{\cotan}{\text{cotan}}
 \newcommand{\somme}[2]{\sum_{#1=0}^{#2}}
 \newcommand{\Vect}[1]{\text{vect}\pth{#1}}
 \newcommand{\Tr}{\text{Tr}}
 \newcommand{\ie}{\emph{ie} } 
\newcommand{\transp}{{}^t\!}
 \newcommand{\lleq}{<\!\!\!<}


 % pour les intégrales :
 \newcommand{\bigcro}[1]{\big[#1\big]}
 \newcommand{\de}{\,\mathrm{d}}


 % Les ensembles :
 \newcommand{\Ce}{\mathbb{C}}
 \newcommand{\Er}{\mathbb{R}}
 \newcommand{\En}{\mathbb{N}}
 \newcommand{\Zed}{\mathbb{Z}}
 \newcommand{\Qu}{\mathbb{Q}}
 \newcommand{\Te}{\mathbb{T}}

 % En probas
 \newcommand{\prb}[1]{\mathbb{P}\pth{#1}}
\newcommand{\Esp}[1]{\mathbb{E}\cro{#1}}
 \newcommand{\Var}[1]{\text{Var}\pth{#1}}
\newcommand{\kt }{\,|\,}
\newcommand{\edistr }{\overset{d}{=}}
\newcommand{\cdistr }{\overset{d}{\to}}
\newcommand{\cproba}{\overset{\mathbb{P}}{\to}}
\newcommand{\easrly}{\overset{a.s.}{=}}
\newcommand{\casrly}{\overset{a.s.}{\to}}

% spécifique

\newcommand{\dr}{\partial}
\newcommand{\fr}{\mathcal{F}}


 \title{Notes sur le projet}

 \author{Jérémy Andréoletti et Nathanaël Boutillon}
 
\begin{document}

\maketitle

\section{Problème à résoudre}

Expérimentalement, on est capable de mesurer approximativement la distribution de la fitness au temps $t$, $W_t$, et ce pour tout $t\geqslant 0$. On est également capable de montrer que le nombre de mutations $N_t$ est un processus de Poisson. Nous voulons déduire de ces deux informations la distribution des effets des mutations sur la fitness.

On note $t_i$ l'instant de la mutation $i$, et on définit l'effet relatif $s_i$ de la mutation $i$ ainsi: \[s_i=\frac{W_{t_{i-1}}-W_{t_i}}{W_{t_{i-1}}}\] Ainsi, $s_i<0$ si la mutation est bénéfique, $s_i>0$ si la mutation est délétère et $s_i=1$ si la mutation est létale. On fait l'hypothèse que les mutations sont iid, ce qui est justifié par une expérience. On supposera également, sans perte de généralité, que $W_0=1$.

On a, pour tout $t\geqslant 0$: \[W_t=\prod_{i=1}^{N_t}(1-s_i)\]

On aimerait, avec cette expression, sachant la loi de $W_t$ et de $N_t$, trouver la loi des $s_i$.

\subsection{Résolution par les moments}

On a, par indépendance, pour $k\geqslant 1$:\[\Esp{W_t^k}=\Esp{\prod_{i=1}^{N_t}(1-s_i)^k}=e^{\lambda t\pth{\Esp{(1-s)^k}-1}}\] donc:
\[\Esp{(1-s)^k}=\frac{1}{\lambda t}\pth{1+\ln\Esp{W_t^k}}\]

Comme nous sommes capables d'estimer les $\Esp{W_t^k}$, on peut estimer les $\Esp{(1-s)^k}$. A priori, cela nous permet de trouver la distribution $f$ de $X=1-s$. Cependant, les approximations nécessaires peuvent se révéler très embêtantes.

\paragraph{Méthode par la fonction caractéristique (qui ne marche sans doute pas)}

À partir de tous les moments, on peut calculer la fonction caractéristique
\[\varphi_X(\xi):=\Esp{e^{i\xi X}}=\Esp{\sum_{k=0}^{+\infty}\frac{(iX\xi)^k}{k!}}=\underbrace{\sum_{k=0}^{N}\frac{(i\xi)^k}{k!}\Esp{X^k}}_{\hat{\varphi}_X(\xi)}}+\Esp{\sum_{k=N+1}^{+\infty}\frac{(iX\xi)^k}{k!}}\]%=?\sum_{k=0}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}\]

À partir de la fonction caractéristique $\varphi_X$, on peut trouver la distribution de $X$, $f$. Seulement, on ne connaît que les $N>0$ premiers moments, chacun avec une certaine erreur $(\varepsilon_k)_{1\leqslant k\leqslant N}$. Ainsi, on ne va calculer $\varphi_X(\xi)$ avec une erreur raisonnable que pour $\xi\leqslant A$, ce qui induira une erreur sur l'estimation de $f$.

Notons $\hat{f}$ la fonction que l'on calcule par cette méthode, qui est une estimation de $f$. On a:
\begin{align*}
  \hat{f}(x)&=\int_{-A}^A\pth{\sum_{k=0}^{N}\frac{(i\xi)^k}{k!}m_k}e^{i\xi x}\de\xi=
\end{align*} pour $A>0$, $m_k$ notre estimation du moment d'ordre $k$, $N$ l'ordre du moment maximal qu'on peut calculer.
et donc:
\begin{align*}
  \abs{f(x)-\hat{f}(x)}&=\abs{\int_{\Er}\pth{\sum_{k=0}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}e^{i\xi x}}\de\xi-\int_{-A}^A\pth{\sum_{k=0}^{N}\frac{(i\xi)^k}{k!}m_k}e^{i\xi x}\de\xi}\\
  &\leqslant\underbrace{\abs{\int_{\abs{x}>A}\pth{\sum_{k=0}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}e^{i\xi x}}\de\xi}}_{\alpha_1}+\underbrace{\abs{\int_{-A}^A\pth{\sum_{k=N+1}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}}e^{i\xi x}\de\xi}}_{\alpha_2}\\
  &+\underbrace{\abs{\int_{-A}^A\pth{\sum_{k=0}^{N}\frac{(i\xi)^k}{k!}\abs{\Esp{X^k}-m_k}}e^{i\xi x}\de\xi}}_{\alpha_3}
\end{align*}


Par calcul, on peut montrer que, pour $x\in\Er$:
\[\abs{f(x)-\hat{f}(x)}\leqslant \alpha_1+\alpha_2+\alpha_3\]
où
\begin{itemize}
\item $\alpha_1$ est l'erreur que l'on commet en omettant de calculer $\varphi_X(\xi)$ pour $\xi>A$ (erreur de régularisation). On a:
  \begin{align*}
    \alpha_1&\leqslant \int_{\abs{x}>A}\abs{\varphi_X(\xi)e^{ix\xi}}\de\xi=\int_{\abs{x}>A}\abs{\varphi_X(\xi)}\de\xi
  \end{align*}
  Or, pour tout $k\geqslant 1$: $\pth{\mathcal{F}(f^{(k)})}(\xi)&=(i\xi)^k\varphi(\xi)$ donc:
  \begin{align*}
    \abs{\int_{\xi>A}\varphi_X(\xi)e^{ix\xi}\de\xi}&=\abs{\int_{\xi>A}\frac{1}{(i\xi)^k}\mathcal{F}(f^{(k)})(\xi)e^{ix\xi}\de\xi}\\
    &\leqslant \int_{\xi>A}\frac{1}{\abs{\xi}^k}\underbrace{\abs{\mathcal{F}(f^{(k)})(\xi)}}_{\leqslant\dabs{f^{(k)}}_{1}}\de\xi\\
    &\leqslant 2\dabs{f^{(k)}}_{1}\int_{\xi>A}1/(\xi^k)\de\xi
  \end{align*}

  On a donc, pour tout $k\geqslant 1$:
  \[\alpha_1\leqslant\frac{2\dabs{f^{(k)}}_{1}}{(k-1)A^{k-1}}\] Pour que cette borne soit bonne, il faut d'une part faire certaines hypothèse sur $f$, d'autre part prendre $A$ assez grand;
\item $\alpha_2$ est l'erreur que l'on commet en omettant dans notre calcul les moments d'ordre plus grand que $N+1$. On a :
  \begin{align*}
    \alpha_2&\leqslant \int_{-A}^A\abs{\sum_{k=N+1}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}}e^{i\xi x}\de\xi\\
    &\leqslant  \int_{-A}^A\sum_{k=N+1}^{+\infty}\frac{\abs{\xi}^k}{k!}\Esp{X^k}\de\xi=2 \int_{0}^A\sum_{k=N+1}^{+\infty}\frac{\xi^k}{k!}\Esp{X^k}\de\xi\\
    &\leqslant 2\sum_{k=N+1}^{+\infty}\frac{A^{k+1}}{(k+1)!}\Esp{X^k}=2\Esp{\frac{1}{X}\sum_{k=N+1}^{+\infty}\frac{(AX)^{k+1}}{(k+1)!}}\\
  \end{align*}
  D'après la formule de Taylor avec reste intégral:
  \begin{align*}
    \sum_{k=N+2}^{+\infty}\frac{(AX)^{k}}{k!}&=e^{AX}-\sum_{k=0}^{N+1}\frac{(AX)^{k}}{k!}\\
    &=\sum_{k=0}^{N+1}\frac{(AX)^{k}}{k!}+\int_{0}^{AX}\frac{(AX-t)^{N+1}e^t}{(N+1)!}\de t-\sum_{k=0}^{N+1}\frac{(AX)^{k}}{k!}\\
    &=\int_{0}^{AX}\frac{(AX-t)^{N+1}e^t}{(N+1)!}\de t
  \end{align*}
  d'où:
  \begin{align*}
    \alpha_2\leqslant 2\Esp{\frac{(AX)^{N+1}e^{AX}}{X(N+1)!}}=\frac{2A^{N+1}}{(N+1)!}\Esp{X^Ne^{AX}}
  \end{align*}

  Pour que cette borne soit bonne, il faut prendre $A$ assez petit et $N$ assez grand;
\item $\alpha_3$ est l'erreur que l'on commet qui provient des erreurs sur le calcul des moments. On a:
  \begin{align*} 
    \alpha_3&\leqslant \int_{-A}^A\sum_{k=0}^{N}\abs{\frac{(i\xi)^k}{k!}\abs{\Esp{X^k}-m_k}e^{i\xi x}}\de\xi\\
    &\leqslant 2\dabs{\varepsilon}_{\infty}\int_{0}^A\sum_{k=0}^{N}\xi^k/(k!)\de\xi\comment{brutal}\\
    &\leqslant 2\dabs{\varepsilon}_{\infty}\int_{0}^Ae^{\xi}\de\xi
  \end{align*}

  On a donc:
  \[\alpha_3\leqslant 2\dabs{\varepsilon}_{\infty}(e^A-1)\]
  C'est une borne qui est assez mauvaise ; cependant, avec des hypothèses sur l'erreur $\varepsilon$, on pourrait sans doute être plus précis. %peut-être obtenir une borne comme: $\alpha_3\leqslant 2(e^{kA\varepsilon_1}-1)$ qui, en prenant $A\lleq 1/(k\varepsilon_1)$, permettrait peut-être d'obtenir une bonne borne sur $\alpha_3$ tout en conservant $A$ assez grand pour avoir une bonne borne sur $\alpha_1$.)
\end{itemize}


\paragraph{Méthode par la transformée de Mellin}

À partir de tous les moments, on peut calculer la transformée de Mellin
\[\varphi_X(\xi):=\Esp{e^{i\xi X}}=\Esp{\sum_{k=0}^{+\infty}\frac{(iX\xi)^k}{k!}}=\underbrace{\sum_{k=0}^{N}\frac{(i\xi)^k}{k!}\Esp{X^k}}_{\hat{\varphi}_X(\xi)}}+\Esp{\sum_{k=N+1}^{+\infty}\frac{(iX\xi)^k}{k!}}\]%=?\sum_{k=0}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}\]

À partir de la fonction caractéristique $\varphi_X$, on peut trouver la distribution de $X$, $f$. Seulement, on ne connaît que les $N>0$ premiers moments, chacun avec une certaine erreur $(\varepsilon_k)_{1\leqslant k\leqslant N}$. Ainsi, on ne va calculer $\varphi_X(\xi)$ avec une erreur raisonnable que pour $\xi\leqslant A$, ce qui induira une erreur sur l'estimation de $f$.

Notons $\hat{f}$ la fonction que l'on calcule par cette méthode, qui est une estimation de $f$. On a:
\begin{align*}
  \hat{f}(x)&=\int_{-A}^A\pth{\sum_{k=0}^{N}\frac{(i\xi)^k}{k!}m_k}e^{i\xi x}\de\xi=
\end{align*} pour $A>0$, $m_k$ notre estimation du moment d'ordre $k$, $N$ l'ordre du moment maximal qu'on peut calculer.
et donc:
\begin{align*}
  \abs{f(x)-\hat{f}(x)}&=\abs{\int_{\Er}\pth{\sum_{k=0}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}e^{i\xi x}}\de\xi-\int_{-A}^A\pth{\sum_{k=0}^{N}\frac{(i\xi)^k}{k!}m_k}e^{i\xi x}\de\xi}\\
  &\leqslant\underbrace{\abs{\int_{\abs{x}>A}\pth{\sum_{k=0}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}e^{i\xi x}}\de\xi}}_{\alpha_1}+\underbrace{\abs{\int_{-A}^A\pth{\sum_{k=N+1}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}}e^{i\xi x}\de\xi}}_{\alpha_2}\\
  &+\underbrace{\abs{\int_{-A}^A\pth{\sum_{k=0}^{N}\frac{(i\xi)^k}{k!}\abs{\Esp{X^k}-m_k}}e^{i\xi x}\de\xi}}_{\alpha_3}
\end{align*}


Par calcul, on peut montrer que, pour $x\in\Er$:
\[\abs{f(x)-\hat{f}(x)}\leqslant \alpha_1+\alpha_2+\alpha_3\]
où
\begin{itemize}
\item $\alpha_1$ est l'erreur que l'on commet en omettant de calculer $\varphi_X(\xi)$ pour $\xi>A$ (erreur de régularisation). On a:
  \begin{align*}
    \alpha_1&\leqslant \int_{\abs{x}>A}\abs{\varphi_X(\xi)e^{ix\xi}}\de\xi=\int_{\abs{x}>A}\abs{\varphi_X(\xi)}\de\xi
  \end{align*}
  Or, pour tout $k\geqslant 1$: $\pth{\mathcal{F}(f^{(k)})}(\xi)&=(i\xi)^k\varphi(\xi)$ donc:
  \begin{align*}
    \abs{\int_{\xi>A}\varphi_X(\xi)e^{ix\xi}\de\xi}&=\abs{\int_{\xi>A}\frac{1}{(i\xi)^k}\mathcal{F}(f^{(k)})(\xi)e^{ix\xi}\de\xi}\\
    &\leqslant \int_{\xi>A}\frac{1}{\abs{\xi}^k}\underbrace{\abs{\mathcal{F}(f^{(k)})(\xi)}}_{\leqslant\dabs{f^{(k)}}_{1}}\de\xi\\
    &\leqslant 2\dabs{f^{(k)}}_{1}\int_{\xi>A}1/(\xi^k)\de\xi
  \end{align*}

  On a donc, pour tout $k\geqslant 1$:
  \[\alpha_1\leqslant\frac{2\dabs{f^{(k)}}_{1}}{(k-1)A^{k-1}}\] Pour que cette borne soit bonne, il faut d'une part faire certaines hypothèse sur $f$, d'autre part prendre $A$ assez grand;
\item $\alpha_2$ est l'erreur que l'on commet en omettant dans notre calcul les moments d'ordre plus grand que $N+1$. On a :
  \begin{align*}
    \alpha_2&\leqslant \int_{-A}^A\abs{\sum_{k=N+1}^{+\infty}\frac{(i\xi)^k}{k!}\Esp{X^k}}e^{i\xi x}\de\xi\\
    &\leqslant  \int_{-A}^A\sum_{k=N+1}^{+\infty}\frac{\abs{\xi}^k}{k!}\Esp{X^k}\de\xi=2 \int_{0}^A\sum_{k=N+1}^{+\infty}\frac{\xi^k}{k!}\Esp{X^k}\de\xi\\
    &\leqslant 2\sum_{k=N+1}^{+\infty}\frac{A^{k+1}}{(k+1)!}\Esp{X^k}=2\Esp{\frac{1}{X}\sum_{k=N+1}^{+\infty}\frac{(AX)^{k+1}}{(k+1)!}}\\
  \end{align*}
  D'après la formule de Taylor avec reste intégral:
  \begin{align*}
    \sum_{k=N+2}^{+\infty}\frac{(AX)^{k}}{k!}&=e^{AX}-\sum_{k=0}^{N+1}\frac{(AX)^{k}}{k!}\\
    &=\sum_{k=0}^{N+1}\frac{(AX)^{k}}{k!}+\int_{0}^{AX}\frac{(AX-t)^{N+1}e^t}{(N+1)!}\de t-\sum_{k=0}^{N+1}\frac{(AX)^{k}}{k!}\\
    &=\int_{0}^{AX}\frac{(AX-t)^{N+1}e^t}{(N+1)!}\de t
  \end{align*}
  d'où:
  \begin{align*}
    \alpha_2\leqslant 2\Esp{\frac{(AX)^{N+1}e^{AX}}{X(N+1)!}}=\frac{2A^{N+1}}{(N+1)!}\Esp{X^Ne^{AX}}
  \end{align*}

  Pour que cette borne soit bonne, il faut prendre $A$ assez petit et $N$ assez grand;
\item $\alpha_3$ est l'erreur que l'on commet qui provient des erreurs sur le calcul des moments. On a:
  \begin{align*} 
    \alpha_3&\leqslant \int_{-A}^A\sum_{k=0}^{N}\abs{\frac{(i\xi)^k}{k!}\abs{\Esp{X^k}-m_k}e^{i\xi x}}\de\xi\\
    &\leqslant 2\dabs{\varepsilon}_{\infty}\int_{0}^A\sum_{k=0}^{N}\xi^k/(k!)\de\xi\comment{brutal}\\
    &\leqslant 2\dabs{\varepsilon}_{\infty}\int_{0}^Ae^{\xi}\de\xi
  \end{align*}

  On a donc:
  \[\alpha_3\leqslant 2\dabs{\varepsilon}_{\infty}(e^A-1)\]
  C'est une borne qui est assez mauvaise ; cependant, avec des hypothèses sur l'erreur $\varepsilon$, on pourrait sans doute être plus précis. %peut-être obtenir une borne comme: $\alpha_3\leqslant 2(e^{kA\varepsilon_1}-1)$ qui, en prenant $A\lleq 1/(k\varepsilon_1)$, permettrait peut-être d'obtenir une bonne borne sur $\alpha_3$ tout en conservant $A$ assez grand pour avoir une bonne borne sur $\alpha_1$.)
\end{itemize}

\subsection{Choses sur les EDP}

\paragraph{Introduction du modèle}

On a \[\ln W_t=\sum_{i=1}^{N_t}\ln(1-s_i)\]

Posons $u(t)\in C^{\infty}(\Er)$ la densité de la loi de $\ln W_t$. Posons $f\in C^{\infty}(\Er)$ la densité de la loi de $\ln(1-s)$ sans prendre en compte les mutations létales (on compensera en rajoutant un taux de mortalité dans l'EDP). On peut déduire la DFE à partir de la donnée de $f$ et du taux de mutations létales. Si l'on fait l'hypothèse qu'il n'y a que des mutations délétères, $f$ est à support dans $\Er_-$. Soit $\lambda$ le taux de mutation. Il semble assez naturel d'introduire le modèle:
\[\dr_tu(t,x)=\lambda\pth{\int_{\Er}f(x-y)u(t,y)\de y-\int_{\Er}f(y)u(t,x)\de x}-\mu u(t,x)\] 
En effet, cette expression peut se traduire par:
\begin{align*}
&\esp\text{changement de densité de fitness entre $t$ et $t+\de t$}\\
&=\text{taux de mutations}\pth{\text{gens qui arrivent sur ma fitness}-\text{gens qui partent de ma fitness}}\\
&-\text{gens qui meurent}
\end{align*} 
où $\mu$ est le taux de mortalité (qui comprend la mortalité due à la sénescence et la mortalité due aux mutations létales). 

\paragraph{Calculs}
On a donc, comme $f$ ne contient pas les mutations létales:
\[\dr_tu(t,x)=\lambda\pth{\int_{\Er}f(x-y)u(t,y)\de y-(1-\mu)u(t,x)}-\mu u(t,x)\] 
soit:
\[\dr_tu(t,x)=\lambda (f*u(t))(x)-\pth{\lambda+\mu(1-\lambda)} u(t,x)\] 
que l'on notera:
\[\dr_tu_t(x)=\lambda (f*u_t)(x)-\pth{\lambda+\mu(1-\lambda)} u_t(x)\] 

On connaît $u$ et on aimerait estimer $f$, en sachant que l'EDP ci-dessus est vérifiée (ainsi, on connaît la solution mais on ne connaît pas l'EDP!)

Posons $\sigma=\lambda+\mu(1-\lambda)$. En prenant la transformée de Fourier, on a:
\[\mathcal{F}\pth{\dr_tu_t}(\xi)=\lambda \mathcal{F}f(\xi)\mathcal{F}{u_t}(\xi)-\sigma\mathcal{F}u_t(x)\]
soit:
\[\pth{\dr_t\mathcal{F}u_t}(\xi)=\lambda \mathcal{F}f(\xi)\mathcal{F}{u_t}(\xi)-\sigma\mathcal{F}u_t(x)\]
et donc:
\begin{align*}
\mathcal{F}f(\xi)&=\frac{\dr_t\mathcal{F}u_t(\xi)}{\lambda \mathcal{F}u_t(\xi)}+\frac{\sigma}{\lambda}\\
&=\frac{1}{\lambda}\dr_t\ln\pth{\mathcal{F}u_t(\xi)}+\frac{\sigma}{\lambda}
\end{align*}
Ainsi:
\[f(x)=\frac{1}{\lambda}\mathcal{F}^{-1}\pth{\dr_t\ln\pth{\mathcal{F}u_t(\xi)}+\sigma}\]
ce qui donne une formule explicite pour $f(x)$, à condition que les calculs soient valables...

\paragraph{Vérification 1}
Comme on a, pour tout $\xi\in\Er$:
\[\fr f(\xi)=\frac{1}{\lambda}\dr_t\ln\pth{\mathcal{F}u_t(\xi)}+\sigma/\lambda}\]
on trouve que $\dr_t\ln\pth{\fr u_t}$ est indépendant du temps, c'est-à-dire que pour tout $\xi\in\Er$, le graphe de la fonction \[t\to \ln\pth{\fr u_t(\xi)}=\ln\Esp{e^{-i\xi \ln{W_t}}}\] est une droite de pente $\theta(\xi)$.

\begin{figure}[h]
  \includegraphics[scale=0.4]{joli5.png}
  \includegraphics[scale=0.4]{joli6.png}  
  \caption{Pour les données simulées (en haut) et observées (en bas), on trace le graphe de la fonction $t\to \ln\pth{\fr u_t(\xi)}$ pour différentes valeurs de $\xi$: on s'attend à trouver des droites.}
  
\end{figure}

\paragraph{Vérification 2: Comparaison avec l'autre formule}

Avec d'autres calculs, on trouve:
\[g(x)=\mathcal{F}^{-1}\pth{1+\frac{1}{\lambda t}\Esp{e^{-i\xi Y_t}}}=\mathcal{F}^{-1}\pth{1+\frac{1}{\lambda t}\fr{u_t(\xi)}}=^?\delta_0+\frac{1}{\lambda t}u_t\] où $Y_t=W_t/W_0$ et où $g$ est la densité de $1-s$. Ainsi, en principe, pour $e^y=x>0$,
\[g(x)=f(\ln x)/x \ou f(y)=e^yg(e^y)\]


\[
\dr_tv(t,x)=\lambda\int_{\Er_+}g(y)v(t,x/y)\de y
\]

Vérifions que cela donne la même chose que dans le cas <<~additif~>> (en considérant $\mu=0$ pour simplifier). Posons $x'=e^x$: on a alors $x'v(t,x')=u(t,x)$. Avec le changement de variable $y'=e^y$, on a:
\begin{align*}
  \dr_tv(t,x)=\dr_tu(t,x)&=\int_{\Er_+}\frac{f(\ln y')}{y'}u(t,x-\ln(y'))\de y'\\
  &=\int_{\Er_+}g(y')u(t,\ln(x')-\ln(y'))\de y'\\
  &=\int_{\Er_+}g(y')u(t,\ln(x'/y'))\de y'\\
  &=\int_{\Er_+}\frac{x'}{y'}g(y')v\pth{t,\frac{x'}{y'}}\de y'
\end{align*}
ce qui n'est pas exactement ce que l'on cherchait à obtenir (il y a un terme $x'/y'$ en trop dans l'intégrale...)




\end{document}
